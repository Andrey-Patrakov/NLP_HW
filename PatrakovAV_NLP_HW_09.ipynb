{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e07905e7",
      "metadata": {
        "id": "e07905e7"
      },
      "source": [
        "# Языковое моделирование"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d662181",
      "metadata": {
        "id": "9d662181"
      },
      "source": [
        "## Импорты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f91d13b",
      "metadata": {
        "id": "0f91d13b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c60f55c0",
      "metadata": {
        "id": "c60f55c0"
      },
      "source": [
        "## Настройки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0294bc88",
      "metadata": {
        "id": "0294bc88"
      },
      "outputs": [],
      "source": [
        "PATH = './evgenyi_onegin.txt'\n",
        "\n",
        "SEQ_LENGTH = 100\n",
        "\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 30000\n",
        "\n",
        "RNN_UNITS = 1024\n",
        "EMBEDDING_DIM = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b56227b2",
      "metadata": {
        "id": "b56227b2"
      },
      "source": [
        "## Данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b022174",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b022174",
        "outputId": "2e656c58-6161-40a4-c7fd-a304c0ef1b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "143928\n",
            "Александр Сергеевич Пушкин\n",
            "\n",
            "Евгений Онегин\n",
            "Роман в стихах\n",
            "\n",
            "Не мысля гордый свет забавить,\n",
            "Вниманье дружбы возлюбя,\n",
            "Хотел бы я тебе представить\n",
            "Залог достойнее тебя,\n",
            "Достойнее души прекрасной,\n",
            "Святой исполненной мечты,\n",
            "Поэзии живой и ясной,\n",
            "Высоких дум и простоты;\n",
            "Но так и быть - рукой пристрастной\n",
            "Прими собранье пестрых глав,\n",
            "Полусмешных, полупечальных,\n",
            "Простонародных, идеальных,\n",
            "Небрежный плод моих забав,\n",
            "Бессонниц, легких вдохновений,\n",
            "Незрелых и увядших лет,\n",
            "Ума холодных наблюдений\n",
            "И сердца го\n"
          ]
        }
      ],
      "source": [
        "with open(PATH, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
        "\n",
        "print(len(text))\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "508015d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "508015d9",
        "outputId": "06250f6c-93c8-42a6-c4bf-dd48a1f5513b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "143928 143928\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "char2idx = {char: i for i, char in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[char] for char in text])\n",
        "print(len(text_as_int), len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae590a5",
      "metadata": {
        "id": "1ae590a5"
      },
      "outputs": [],
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(SEQ_LENGTH+1, drop_remainder=True)\n",
        "dataset = sequences.map(split_input_target)\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fc4f3c3",
      "metadata": {
        "id": "7fc4f3c3"
      },
      "source": [
        "## Модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c1ec82d",
      "metadata": {
        "id": "2c1ec82d"
      },
      "outputs": [],
      "source": [
        "# без задания батч-сайза\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "                                 \n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=False,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=False,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "         tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=False,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "                                   \n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b36332cc",
      "metadata": {
        "id": "b36332cc"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "675d2cb5",
      "metadata": {
        "id": "675d2cb5"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "model = build_model(vocab_size, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136a88d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "136a88d8",
        "outputId": "d7c124b1-8628-4e2a-f964-28625fdd3f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "44/44 [==============================] - 19s 176ms/step - loss: 3.8499\n",
            "Epoch 2/15\n",
            "44/44 [==============================] - 7s 146ms/step - loss: 3.0091\n",
            "Epoch 3/15\n",
            "44/44 [==============================] - 6s 122ms/step - loss: 2.5826\n",
            "Epoch 4/15\n",
            "44/44 [==============================] - 6s 124ms/step - loss: 2.4253\n",
            "Epoch 5/15\n",
            "44/44 [==============================] - 6s 131ms/step - loss: 2.3076\n",
            "Epoch 6/15\n",
            "44/44 [==============================] - 6s 122ms/step - loss: 2.2023\n",
            "Epoch 7/15\n",
            "44/44 [==============================] - 6s 122ms/step - loss: 2.0874\n",
            "Epoch 8/15\n",
            "44/44 [==============================] - 6s 122ms/step - loss: 1.9657\n",
            "Epoch 9/15\n",
            "44/44 [==============================] - 6s 122ms/step - loss: 1.8274\n",
            "Epoch 10/15\n",
            "44/44 [==============================] - 6s 123ms/step - loss: 1.6820\n",
            "Epoch 11/15\n",
            "44/44 [==============================] - 6s 121ms/step - loss: 1.5229\n",
            "Epoch 12/15\n",
            "44/44 [==============================] - 6s 122ms/step - loss: 1.3609\n",
            "Epoch 13/15\n",
            "44/44 [==============================] - 6s 122ms/step - loss: 1.1749\n",
            "Epoch 14/15\n",
            "44/44 [==============================] - 6s 122ms/step - loss: 0.9737\n",
            "Epoch 15/15\n",
            "44/44 [==============================] - 6s 122ms/step - loss: 0.7696\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7724597f",
      "metadata": {
        "id": "7724597f"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string):\n",
        "  \n",
        "    num_generate = 512\n",
        "    input_eval = [char2idx[char] for char in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "  \n",
        "    temperature = 0.5\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "       \n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6d1600",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea6d1600",
        "outputId": "70c58ddd-03ed-44e4-cc0d-78b9159c8fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "528\n",
            "Случайный текст в в хокрадододо слала но до доводонараралине ве мнето ви ве пой пой пой хотодовай по о о дамисой сто пе мне возв вымлитовой ходамной ный от,\n",
            "Пе желе ми во боде пле сть поче далоть вой м вой ветотой в,\n",
            "И стовой наво воклинозакотобоной хотодотой мны наль сокодей ноде от в ой блой ны\n",
            "Пой вымара пой и дастой подракакасторе пой ви о отася водна ной о залодалой дарить оде сям,\n",
            "Водо бый однаритой онареть ои оде хой вой сцы но накой о ся вовымнетоть вой вокотой в де вить ви нудралисодрестова пралы ся ло драпой м,\n",
            "Ч\n"
          ]
        }
      ],
      "source": [
        "gen_text = generate_text(model, start_string=\"Случайный текст \")\n",
        "print(len(gen_text), gen_text, sep='\\n')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}